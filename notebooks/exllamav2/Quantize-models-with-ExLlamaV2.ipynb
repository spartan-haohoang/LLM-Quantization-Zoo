{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ExLlamaV2: High-Performance LLM Quantization\n",
        "\n",
        "This notebook demonstrates quantization of large language models using ExLlamaV2, a high-performance library for running LLMs efficiently.\n",
        "\n",
        "## Overview\n",
        "- **Quantization Method**: ExLlamaV2 format\n",
        "- **Use Case**: Fast inference with reduced memory footprint\n",
        "- **Target**: Large language models (7B+ parameters)\n",
        "\n",
        "## Key Features\n",
        "- Efficient quantization with calibration dataset\n",
        "- Configurable bits per weight (BPW)\n",
        "- Optimized for inference speed\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Install ExLlamaV2\n",
        "\n",
        "First, we'll clone and install the ExLlamaV2 library from its GitHub repository.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install ExLLamaV2\n",
        "!git clone https://github.com/turboderp/exllamav2\n",
        "!pip install -e exllamav2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Configure Model and Quantization Parameters\n",
        "\n",
        "Set the model name and bits per weight (BPW) for quantization. Lower BPW values result in smaller models but may reduce quality.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "MODEL_NAME = \"zephyr-7b-beta\"\n",
        "BPW = 5.0  # Bits per weight - adjust based on your memory constraints\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Download Base Model\n",
        "\n",
        "Download the model from HuggingFace and prepare it for quantization.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download model from HuggingFace\n",
        "!git lfs install\n",
        "!git clone https://huggingface.co/HuggingFaceH4/{MODEL_NAME}\n",
        "!mv {MODEL_NAME} base_model\n",
        "!rm -f base_model/*.bin  # Remove unnecessary .bin files\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Download Calibration Dataset\n",
        "\n",
        "Download the WikiText dataset for calibration during quantization.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download calibration dataset\n",
        "!wget https://huggingface.co/datasets/wikitext/resolve/9a9e482b5987f9d25b3a9b2883fc6cc9fd8071b3/wikitext-103-v1/wikitext-test.parquet\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Quantize the Model\n",
        "\n",
        "Run the quantization process using the specified BPW and calibration dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Quantize the model\n",
        "!mkdir quant\n",
        "!python exllamav2/convert.py \\\n",
        "    -i base_model \\\n",
        "    -o quant \\\n",
        "    -c wikitext-test.parquet \\\n",
        "    -b {BPW}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Prepare Quantized Model\n",
        "\n",
        "Copy necessary files and clean up the output directory.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clean up and copy necessary files\n",
        "!rm -rf quant/out_tensor\n",
        "!rsync -av --exclude='*.safetensors' --exclude='.*' ./base_model/ ./quant/\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Test the Quantized Model\n",
        "\n",
        "Run inference with the quantized model to verify it works correctly.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test inference with the quantized model\n",
        "!python exllamav2/test_inference.py -m quant/ -p \"I have a dream\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 8: Upload to HuggingFace Hub (Optional)\n",
        "\n",
        "Upload the quantized model to HuggingFace for sharing and deployment.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages for uploading\n",
        "!pip install -q huggingface_hub\n",
        "!git config --global credential.helper store\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from huggingface_hub import notebook_login, HfApi\n",
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
        "\n",
        "# Login to HuggingFace\n",
        "notebook_login()\n",
        "\n",
        "# Initialize API\n",
        "api = HfApi()\n",
        "\n",
        "# Replace 'your-username' with your HuggingFace username\n",
        "username = \"your-username\"\n",
        "\n",
        "# Create repository\n",
        "api.create_repo(\n",
        "    repo_id=f\"{username}/{MODEL_NAME}-{BPW:.1f}bpw-exl2\",\n",
        "    repo_type=\"model\",\n",
        "    exist_ok=True\n",
        ")\n",
        "\n",
        "# Upload the quantized model\n",
        "api.upload_folder(\n",
        "    repo_id=f\"{username}/{MODEL_NAME}-{BPW:.1f}bpw-exl2\",\n",
        "    folder_path=\"quant\",\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
