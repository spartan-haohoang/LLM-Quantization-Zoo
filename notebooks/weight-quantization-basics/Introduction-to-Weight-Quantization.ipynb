{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Introduction to Weight Quantization\n",
        "## Reducing Large Language Model Size with 8-bit Quantization\n",
        "\n",
        "This notebook provides a comprehensive introduction to weight quantization techniques for Large Language Models (LLMs).\n",
        "\n",
        "## Overview\n",
        "- **Goal**: Understand fundamental quantization techniques\n",
        "- **Methods Covered**: Absmax, Zero-point, and LLM.int8()\n",
        "- **Benefits**: Reduced memory footprint, faster inference\n",
        "- **Use Case**: Educational and practical applications\n",
        "\n",
        "## What is Quantization?\n",
        "Quantization reduces the precision of model weights from high-precision formats (e.g., FP32, FP16) to lower-precision formats (e.g., INT8), significantly reducing model size while maintaining acceptable performance.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 1: Basic Quantization Functions\n",
        "\n",
        "We'll implement two fundamental quantization methods:\n",
        "1. **Absmax Quantization**: Symmetric quantization using absolute maximum\n",
        "2. **Zero-point Quantization**: Asymmetric quantization with zero-point offset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "def absmax_quantize(X):\n",
        "    \"\"\"\n",
        "    Absmax quantization: Symmetric quantization method\n",
        "    Maps values to [-127, 127] range using absolute maximum\n",
        "    \"\"\"\n",
        "    # Calculate scale factor\n",
        "    scale = 127 / torch.max(torch.abs(X))\n",
        "    \n",
        "    # Quantize: scale and round\n",
        "    X_quant = (scale * X).round()\n",
        "    \n",
        "    # Dequantize: reverse the scaling\n",
        "    X_dequant = X_quant / scale\n",
        "    \n",
        "    return X_quant.to(torch.int8), X_dequant\n",
        "\n",
        "\n",
        "def zeropoint_quantize(X):\n",
        "    \"\"\"\n",
        "    Zero-point quantization: Asymmetric quantization method\n",
        "    Maps values to [-128, 127] range with zero-point offset\n",
        "    \"\"\"\n",
        "    # Calculate value range\n",
        "    x_range = torch.max(X) - torch.min(X)\n",
        "    x_range = 1 if x_range == 0 else x_range\n",
        "    \n",
        "    # Calculate scale factor\n",
        "    scale = 255 / x_range\n",
        "    \n",
        "    # Calculate zero-point offset\n",
        "    zeropoint = (-scale * torch.min(X) - 128).round()\n",
        "    \n",
        "    # Quantize: scale, shift, and clip\n",
        "    X_quant = torch.clip((X * scale + zeropoint).round(), -128, 127)\n",
        "    \n",
        "    # Dequantize: reverse shift and scaling\n",
        "    X_dequant = (X_quant - zeropoint) / scale\n",
        "    \n",
        "    return X_quant.to(torch.int8), X_dequant\n",
        "\n",
        "print(\"✓ Quantization functions defined\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 2: Install Dependencies\n",
        "\n",
        "Install required libraries for model quantization.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install -q bitsandbytes>=0.39.0\n",
        "!pip install -q git+https://github.com/huggingface/accelerate.git\n",
        "!pip install -q git+https://github.com/huggingface/transformers.git\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 3: Load Model and Extract Weights\n",
        "\n",
        "Load GPT-2 model and examine its weights.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "torch.manual_seed(0)\n",
        "\n",
        "# Set device\n",
        "device = 'cpu'\n",
        "\n",
        "# Load model and tokenizer\n",
        "model_id = 'gpt2'\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id).to(device)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "# Print model size\n",
        "print(f\"Model size: {model.get_memory_footprint():,} bytes\")\n",
        "print(f\"Model size: {model.get_memory_footprint() / 1e6:.2f} MB\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 4: Apply Quantization to Model Weights\n",
        "\n",
        "Extract weights from the first layer and apply both quantization methods.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract weights from first attention layer\n",
        "weights = model.transformer.h[0].attn.c_attn.weight.data\n",
        "print(\"Original weights:\")\n",
        "print(weights)\n",
        "print(f\"\\nShape: {weights.shape}\")\n",
        "print(f\"Data type: {weights.dtype}\")\n",
        "\n",
        "# Apply absmax quantization\n",
        "weights_abs_quant, weights_abs_dequant = absmax_quantize(weights)\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Absmax quantized weights:\")\n",
        "print(weights_abs_quant)\n",
        "print(f\"Data type: {weights_abs_quant.dtype}\")\n",
        "\n",
        "# Apply zero-point quantization\n",
        "weights_zp_quant, weights_zp_dequant = zeropoint_quantize(weights)\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Zero-point quantized weights:\")\n",
        "print(weights_zp_quant)\n",
        "print(f\"Data type: {weights_zp_quant.dtype}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 5: Quantize Entire Model\n",
        "\n",
        "Apply quantization to all model parameters.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from copy import deepcopy\n",
        "\n",
        "# Store original weights\n",
        "weights = [param.data.clone() for param in model.parameters()]\n",
        "\n",
        "# Create model with absmax quantization\n",
        "model_abs = deepcopy(model)\n",
        "weights_abs = []\n",
        "print(\"Applying absmax quantization to all layers...\")\n",
        "for param in model_abs.parameters():\n",
        "    _, dequantized = absmax_quantize(param.data)\n",
        "    param.data = dequantized\n",
        "    weights_abs.append(dequantized)\n",
        "print(\"✓ Absmax quantization complete\")\n",
        "\n",
        "# Create model with zero-point quantization\n",
        "model_zp = deepcopy(model)\n",
        "weights_zp = []\n",
        "print(\"Applying zero-point quantization to all layers...\")\n",
        "for param in model_zp.parameters():\n",
        "    _, dequantized = zeropoint_quantize(param.data)\n",
        "    param.data = dequantized\n",
        "    weights_zp.append(dequantized)\n",
        "print(\"✓ Zero-point quantization complete\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 6: Visualize Weight Distributions\n",
        "\n",
        "Compare the distributions of original and quantized weights.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "# Flatten weight tensors for visualization\n",
        "weights_flat = np.concatenate([t.cpu().numpy().flatten() for t in weights])\n",
        "weights_abs_flat = np.concatenate([t.cpu().numpy().flatten() for t in weights_abs])\n",
        "weights_zp_flat = np.concatenate([t.cpu().numpy().flatten() for t in weights_zp])\n",
        "\n",
        "# Set plot style\n",
        "plt.style.use('ggplot')\n",
        "\n",
        "# Create figure with two subplots\n",
        "fig, axs = plt.subplots(2, figsize=(10, 10), dpi=300, sharex=True)\n",
        "\n",
        "# Plot 1: Original vs Absmax\n",
        "axs[0].hist(weights_flat, bins=150, alpha=0.5, label='Original weights', \n",
        "            color='blue', range=(-2, 2))\n",
        "axs[0].hist(weights_abs_flat, bins=150, alpha=0.5, label='Absmax weights', \n",
        "            color='red', range=(-2, 2))\n",
        "\n",
        "# Plot 2: Original vs Zero-point\n",
        "axs[1].hist(weights_flat, bins=150, alpha=0.5, label='Original weights', \n",
        "            color='blue', range=(-2, 2))\n",
        "axs[1].hist(weights_zp_flat, bins=150, alpha=0.5, label='Zero-point weights', \n",
        "            color='green', range=(-2, 2))\n",
        "\n",
        "# Add grid and legends\n",
        "for ax in axs:\n",
        "    ax.grid(True, linestyle='--', alpha=0.6)\n",
        "    ax.legend()\n",
        "    ax.set_ylabel('Count', fontsize=14)\n",
        "    ax.yaxis.set_major_formatter(ticker.EngFormatter())\n",
        "\n",
        "# Add titles and labels\n",
        "axs[0].set_title('Comparison of Original and Absmax Quantized Weights', fontsize=16)\n",
        "axs[1].set_title('Comparison of Original and Zero-point Quantized Weights', fontsize=16)\n",
        "axs[1].set_xlabel('Weight Values', fontsize=14)\n",
        "\n",
        "plt.rc('font', size=12)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"Total weights visualized: {len(weights_flat):,}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 7: Test Generation Quality\n",
        "\n",
        "Generate text with original and quantized models to compare output quality.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_text(model, input_text, max_length=50):\n",
        "    \"\"\"Generate text using the model\"\"\"\n",
        "    input_ids = tokenizer.encode(input_text, return_tensors='pt').to(device)\n",
        "    output = model.generate(\n",
        "        inputs=input_ids,\n",
        "        max_length=max_length,\n",
        "        do_sample=True,\n",
        "        top_k=30,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "        attention_mask=input_ids.new_ones(input_ids.shape)\n",
        "    )\n",
        "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "# Generate text with each model\n",
        "prompt = \"I have a dream\"\n",
        "print(f\"Prompt: '{prompt}'\\n\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "original_text = generate_text(model, prompt)\n",
        "print(\"Original model:\")\n",
        "print(original_text)\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "\n",
        "absmax_text = generate_text(model_abs, prompt)\n",
        "print(\"Absmax quantized model:\")\n",
        "print(absmax_text)\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "\n",
        "zp_text = generate_text(model_zp, prompt)\n",
        "print(\"Zero-point quantized model:\")\n",
        "print(zp_text)\n",
        "print(\"=\"*70)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 8: Calculate Perplexity\n",
        "\n",
        "Evaluate model quality using perplexity metric.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_perplexity(model, text):\n",
        "    \"\"\"Calculate perplexity of generated text\"\"\"\n",
        "    # Encode the text\n",
        "    encodings = tokenizer(text, return_tensors='pt').to(device)\n",
        "    \n",
        "    # Define input and target ids\n",
        "    input_ids = encodings.input_ids\n",
        "    target_ids = input_ids.clone()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids, labels=target_ids)\n",
        "    \n",
        "    # Calculate perplexity from loss\n",
        "    neg_log_likelihood = outputs.loss\n",
        "    ppl = torch.exp(neg_log_likelihood)\n",
        "    \n",
        "    return ppl\n",
        "\n",
        "# Calculate perplexity for each model\n",
        "ppl = calculate_perplexity(model, original_text)\n",
        "ppl_abs = calculate_perplexity(model_abs, original_text)\n",
        "ppl_zp = calculate_perplexity(model_zp, original_text)\n",
        "\n",
        "print(\"Perplexity Comparison:\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Original model:       {ppl.item():.2f}\")\n",
        "print(f\"Absmax quantized:     {ppl_abs.item():.2f}\")\n",
        "print(f\"Zero-point quantized: {ppl_zp.item():.2f}\")\n",
        "print(\"=\"*50)\n",
        "print(\"\\nNote: Lower perplexity indicates better model quality\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 9: LLM.int8() - Advanced 8-bit Quantization\n",
        "\n",
        "Use bitsandbytes library for production-ready 8-bit quantization.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check device availability\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Load model with 8-bit quantization\n",
        "model_int8 = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    device_map='auto',\n",
        "    load_in_8bit=True,\n",
        ")\n",
        "\n",
        "# Compare model sizes\n",
        "original_size = model.get_memory_footprint()\n",
        "int8_size = model_int8.get_memory_footprint()\n",
        "\n",
        "print(\"\\nModel Size Comparison:\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Original model: {original_size:,} bytes ({original_size / 1e6:.2f} MB)\")\n",
        "print(f\"LLM.int8() model: {int8_size:,} bytes ({int8_size / 1e6:.2f} MB)\")\n",
        "print(f\"Compression ratio: {original_size / int8_size:.2f}x\")\n",
        "print(f\"Size reduction: {(1 - int8_size/original_size)*100:.1f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 10: Visualize LLM.int8() Weights\n",
        "\n",
        "Compare LLM.int8() quantized weights with original weights.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "# Extract int8 weights\n",
        "weights_int8 = [param.data.clone() for param in model_int8.parameters()]\n",
        "weights_int8_flat = np.concatenate([t.cpu().numpy().flatten() for t in weights_int8])\n",
        "\n",
        "# Create visualization\n",
        "plt.style.use('ggplot')\n",
        "fig, ax = plt.subplots(figsize=(10, 5), dpi=300)\n",
        "\n",
        "# Plot histograms\n",
        "ax.hist(weights_flat, bins=150, alpha=0.5, label='Original weights',\n",
        "        color='blue', range=(-2, 2))\n",
        "ax.hist(weights_int8_flat, bins=150, alpha=0.5, label='LLM.int8() weights',\n",
        "        color='red', range=(-2, 2))\n",
        "\n",
        "# Formatting\n",
        "ax.grid(True, linestyle='--', alpha=0.6)\n",
        "ax.legend()\n",
        "ax.set_title('Comparison of Original and LLM.int8() Quantized Weights', fontsize=16)\n",
        "ax.set_xlabel('Weight Values', fontsize=14)\n",
        "ax.set_ylabel('Count', fontsize=14)\n",
        "ax.yaxis.set_major_formatter(ticker.EngFormatter())\n",
        "\n",
        "plt.rc('font', size=12)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 11: Test LLM.int8() Generation Quality\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate text with int8 model\n",
        "text_int8 = generate_text(model_int8, prompt)\n",
        "\n",
        "print(\"Text Generation Comparison:\")\n",
        "print(\"=\"*70)\n",
        "print(\"Original model:\")\n",
        "print(original_text)\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"LLM.int8() model:\")\n",
        "print(text_int8)\n",
        "print(\"=\"*70)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 12: Final Perplexity Comparison\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate perplexity for int8 model\n",
        "ppl_int8 = calculate_perplexity(model_int8, text_int8)\n",
        "\n",
        "print(\"Final Perplexity Comparison:\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Original model:       {ppl.item():.2f}\")\n",
        "print(f\"LLM.int8() model:     {ppl_int8.item():.2f}\")\n",
        "print(\"=\"*50)\n",
        "print(f\"\\nPerplexity difference: {abs(ppl.item() - ppl_int8.item()):.2f}\")\n",
        "print(\"\\nConclusion: LLM.int8() maintains quality while reducing size by ~4x!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "This notebook demonstrated three quantization approaches:\n",
        "\n",
        "1. **Absmax Quantization**: Simple symmetric quantization\n",
        "   - Pros: Fast, simple implementation\n",
        "   - Cons: Less accurate for asymmetric distributions\n",
        "\n",
        "2. **Zero-point Quantization**: Asymmetric quantization with offset\n",
        "   - Pros: Better handling of asymmetric distributions\n",
        "   - Cons: Slightly more complex\n",
        "\n",
        "3. **LLM.int8()**: Production-ready quantization from bitsandbytes\n",
        "   - Pros: Minimal quality loss, significant memory savings\n",
        "   - Cons: Requires specific hardware support\n",
        "\n",
        "### Key Takeaways\n",
        "- Quantization can reduce model size by 4x or more\n",
        "- Modern quantization methods (LLM.int8()) preserve quality remarkably well\n",
        "- Choice of quantization method depends on hardware and quality requirements\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
