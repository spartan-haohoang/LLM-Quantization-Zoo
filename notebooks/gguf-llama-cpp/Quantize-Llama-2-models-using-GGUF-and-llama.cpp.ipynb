{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Quantize Llama 2 Models using GGUF and llama.cpp\n",
        "\n",
        "This notebook demonstrates how to quantize Llama 2 models using the GGUF format and llama.cpp library for efficient inference.\n",
        "\n",
        "## Overview\n",
        "- **Quantization Format**: GGUF (GPT-Generated Unified Format)\n",
        "- **Library**: llama.cpp\n",
        "- **Benefits**: CPU-friendly inference, reduced memory usage, multiple quantization methods\n",
        "\n",
        "## Configuration\n",
        "\n",
        "### Variables\n",
        "* `MODEL_ID`: The HuggingFace model ID to quantize\n",
        "* `QUANTIZATION_METHODS`: List of quantization methods to apply\n",
        "\n",
        "### Recommended Quantization Methods\n",
        "- **Q5_K_M**: Best quality-size tradeoff (recommended for most use cases)\n",
        "- **Q4_K_M**: Good quality with smaller size\n",
        "- **Q6_K**: Highest quality, larger size\n",
        "- **Q3_K_M**: Smaller size, acceptable quality loss\n",
        "- **Q2_K**: Smallest size, significant quality loss (not recommended)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Set Configuration\n",
        "\n",
        "Define the model to quantize and the quantization methods to use.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration\n",
        "MODEL_ID = \"mlabonne/EvolCodeLlama-7b\"\n",
        "QUANTIZATION_METHODS = [\"q4_k_m\", \"q5_k_m\"]\n",
        "\n",
        "# Extract model name\n",
        "MODEL_NAME = MODEL_ID.split('/')[-1]\n",
        "print(f\"Model: {MODEL_NAME}\")\n",
        "print(f\"Quantization methods: {', '.join(QUANTIZATION_METHODS)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Install llama.cpp\n",
        "\n",
        "Clone and compile llama.cpp with CUDA support for GPU acceleration.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clone and build llama.cpp\n",
        "!git clone https://github.com/ggerganov/llama.cpp\n",
        "!cd llama.cpp && git pull && make clean && LLAMA_CUBLAS=1 make\n",
        "!pip install -r llama.cpp/requirements.txt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Download the Model\n",
        "\n",
        "Download the base model from HuggingFace.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download model from HuggingFace\n",
        "!git lfs install\n",
        "!git clone https://huggingface.co/{MODEL_ID}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Convert to FP16\n",
        "\n",
        "First, convert the model to FP16 format, which serves as the base for quantization.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert to FP16 format\n",
        "fp16 = f\"{MODEL_NAME}/{MODEL_NAME.lower()}.fp16.bin\"\n",
        "!python llama.cpp/convert.py {MODEL_NAME} --outtype f16 --outfile {fp16}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Quantize with Multiple Methods\n",
        "\n",
        "Quantize the model using each specified quantization method.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Quantize the model for each method\n",
        "for method in QUANTIZATION_METHODS:\n",
        "    qtype = f\"{MODEL_NAME}/{MODEL_NAME.lower()}.{method.upper()}.gguf\"\n",
        "    print(f\"Quantizing with method: {method}\")\n",
        "    !./llama.cpp/quantize {fp16} {qtype} {method}\n",
        "    print(f\"Created: {qtype}\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Run Inference\n",
        "\n",
        "Test the quantized models with interactive inference.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# List available quantized models\n",
        "model_list = [file for file in os.listdir(MODEL_NAME) if \"gguf\" in file]\n",
        "print(\"Available quantized models:\")\n",
        "for i, model in enumerate(model_list, 1):\n",
        "    print(f\"{i}. {model}\")\n",
        "\n",
        "# Interactive inference\n",
        "prompt = input(\"\\nEnter your prompt: \")\n",
        "chosen_method = input(f\"Name of the model (options: {', '.join(model_list)}): \")\n",
        "\n",
        "# Verify the chosen method is in the list\n",
        "if chosen_method not in model_list:\n",
        "    print(\"Invalid model name!\")\n",
        "else:\n",
        "    model_path = f\"{MODEL_NAME}/{chosen_method}\"\n",
        "    print(f\"\\nRunning inference with {chosen_method}...\")\n",
        "    # Using -ngl 35 to offload all layers to GPU (adjust based on your model)\n",
        "    !./llama.cpp/main -m {model_path} -n 128 --color -ngl 35 -p \"{prompt}\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Upload to HuggingFace Hub (Optional)\n",
        "\n",
        "Upload the quantized models to HuggingFace for sharing and deployment.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -q huggingface_hub\n",
        "\n",
        "from huggingface_hub import create_repo, HfApi\n",
        "from google.colab import userdata\n",
        "\n",
        "# Replace with your HuggingFace username\n",
        "username = \"your-username\"\n",
        "\n",
        "# Use token from Colab secrets or set directly\n",
        "# In Colab: Go to Tools > Secrets and add HF_TOKEN\n",
        "try:\n",
        "    api = HfApi(token=userdata.get(\"HF_TOKEN\"))\n",
        "except:\n",
        "    # Alternative: Login manually\n",
        "    from huggingface_hub import notebook_login\n",
        "    notebook_login()\n",
        "    api = HfApi()\n",
        "\n",
        "# Create repository\n",
        "create_repo(\n",
        "    repo_id=f\"{username}/{MODEL_NAME}-GGUF\",\n",
        "    repo_type=\"model\",\n",
        "    exist_ok=True,\n",
        ")\n",
        "\n",
        "# Upload all GGUF files\n",
        "api.upload_folder(\n",
        "    folder_path=MODEL_NAME,\n",
        "    repo_id=f\"{username}/{MODEL_NAME}-GGUF\",\n",
        "    allow_patterns=\"*.gguf\",\n",
        ")\n",
        "\n",
        "print(f\"âœ“ Models uploaded to: https://huggingface.co/{username}/{MODEL_NAME}-GGUF\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
