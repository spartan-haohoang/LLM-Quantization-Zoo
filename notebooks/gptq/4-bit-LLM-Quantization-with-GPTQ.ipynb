{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 4-bit LLM Quantization with GPTQ\n",
        "\n",
        "This notebook demonstrates GPTQ (Gradient-aware Post-Training Quantization), a sophisticated quantization technique that uses gradient information to minimize accuracy loss.\n",
        "\n",
        "## Overview\n",
        "- **Quantization Method**: GPTQ (Gradient Post-Training Quantization)\n",
        "- **Precision**: 4-bit quantization with grouping\n",
        "- **Benefits**: High compression with minimal accuracy loss\n",
        "- **Use Case**: Production deployments requiring balance of size and quality\n",
        "\n",
        "## Key Parameters\n",
        "- **bits**: Number of bits for quantization (typically 4)\n",
        "- **group_size**: Size of weight groups (128 is common)\n",
        "- **damp_percent**: Damping factor for quantization\n",
        "- **desc_act**: Whether to use activation order descending\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Install Dependencies\n",
        "\n",
        "Install the AutoGPTQ library and transformers.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install AutoGPTQ and required libraries\n",
        "!BUILD_CUDA_EXT=0 pip install -q auto-gptq transformers\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Import Libraries and Configure Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import random\n",
        "from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\n",
        "from datasets import load_dataset\n",
        "import torch\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Define model configuration\n",
        "model_id = \"gpt2\"\n",
        "out_dir = model_id + \"-GPTQ\"\n",
        "\n",
        "print(f\"Model: {model_id}\")\n",
        "print(f\"Output directory: {out_dir}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Load Model and Configure Quantization\n",
        "\n",
        "Set up the quantization configuration and load the model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configure quantization parameters\n",
        "quantize_config = BaseQuantizeConfig(\n",
        "    bits=4,                # 4-bit quantization\n",
        "    group_size=128,        # Group size for quantization\n",
        "    damp_percent=0.01,     # Damping factor\n",
        "    desc_act=False,        # Activation order\n",
        ")\n",
        "\n",
        "# Load model and tokenizer\n",
        "model = AutoGPTQForCausalLM.from_pretrained(model_id, quantize_config)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "print(\"✓ Model and tokenizer loaded\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Prepare Calibration Data\n",
        "\n",
        "Load and prepare the C4 dataset for calibration during quantization.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load calibration dataset\n",
        "n_samples = 1024\n",
        "print(f\"Loading {n_samples} samples from C4 dataset...\")\n",
        "\n",
        "data = load_dataset(\n",
        "    \"allenai/c4\", \n",
        "    data_files=\"en/c4-train.00001-of-01024.json.gz\", \n",
        "    split=f\"train[:{n_samples*5}]\"\n",
        ")\n",
        "\n",
        "# Tokenize the data\n",
        "tokenized_data = tokenizer(\"\\n\\n\".join(data['text']), return_tensors='pt')\n",
        "print(f\"✓ Dataset loaded and tokenized\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Format Training Examples\n",
        "\n",
        "Prepare the calibration examples in the required format.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Format tokenized examples for quantization\n",
        "examples_ids = []\n",
        "for _ in range(n_samples):\n",
        "    i = random.randint(0, tokenized_data.input_ids.shape[1] - tokenizer.model_max_length - 1)\n",
        "    j = i + tokenizer.model_max_length\n",
        "    input_ids = tokenized_data.input_ids[:, i:j]\n",
        "    attention_mask = torch.ones_like(input_ids)\n",
        "    examples_ids.append({'input_ids': input_ids, 'attention_mask': attention_mask})\n",
        "\n",
        "print(f\"✓ Prepared {len(examples_ids)} calibration examples\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Quantize the Model\n",
        "\n",
        "Run the GPTQ quantization process (uncomment to execute).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Quantize the model with GPTQ\n",
        "# Uncomment the following lines to run quantization\n",
        "# This can take significant time depending on model size\n",
        "\n",
        "# %%time\n",
        "# model.quantize(\n",
        "#     examples_ids,\n",
        "#     batch_size=1,\n",
        "#     use_triton=True,\n",
        "# )\n",
        "# \n",
        "# # Save quantized model and tokenizer\n",
        "# model.save_quantized(out_dir, use_safetensors=True)\n",
        "# tokenizer.save_pretrained(out_dir)\n",
        "# print(f\"✓ Model quantized and saved to {out_dir}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Load and Test Quantized Model\n",
        "\n",
        "Load the quantized model and run inference.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Determine device\n",
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Load the quantized model\n",
        "model = AutoGPTQForCausalLM.from_quantized(\n",
        "    out_dir,\n",
        "    device=device,\n",
        "    use_triton=True,\n",
        "    use_safetensors=True,\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(out_dir)\n",
        "\n",
        "print(\"✓ Quantized model loaded successfully\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 8: Run Inference\n",
        "\n",
        "Generate text using the quantized model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Create text generation pipeline\n",
        "generator = pipeline('text-generation', model=model, tokenizer=tokenizer)\n",
        "\n",
        "# Generate text\n",
        "prompt = \"I have a dream\"\n",
        "result = generator(prompt, do_sample=True, max_length=50)[0]['generated_text']\n",
        "\n",
        "print(\"Generated text:\")\n",
        "print(\"-\" * 50)\n",
        "print(result)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
