# ü¶Å LLM Quantization Zoo

<div align="center">

[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-ee4c2c.svg)](https://pytorch.org/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Docker](https://img.shields.io/badge/Docker-Ready-blue.svg)](https://www.docker.com/)

**A comprehensive, production-ready collection of Large Language Model quantization techniques and implementations.**

[Documentation](#documentation) ‚Ä¢ [Quick Start](#-quick-start) ‚Ä¢ [Notebooks](#-notebooks) ‚Ä¢ [Docker](#-docker-support) ‚Ä¢ [Contributing](#contributing)

</div>

---

## üìñ Overview

The **LLM Quantization Zoo** is a curated collection of state-of-the-art quantization techniques for Large Language Models. This repository provides:

- ‚úÖ **4 comprehensive Jupyter notebooks** covering different quantization methods
- ‚úÖ **Production-ready Docker setup** with CUDA support
- ‚úÖ **Poetry & pip dependency management**
- ‚úÖ **Organized project structure** for easy navigation
- ‚úÖ **Detailed documentation** for each method
- ‚úÖ **Helper scripts** for automation

## üéØ Why Quantization?

Quantization reduces model size and inference time by representing weights with lower precision:

| Method | Precision | Size (7B) | Speed | Quality |
|--------|-----------|-----------|-------|---------|
| Original | FP32 | ~28GB | 1x | 100% |
| FP16 | 16-bit | ~14GB | 1.5x | ~100% |
| INT8 | 8-bit | ~7GB | 2-3x | ~99% |
| INT4 | 4-bit | ~3.5GB | 3-4x | ~95-98% |

## üìö Notebooks

### 1. üöÄ [Introduction to Weight Quantization](./notebooks/weight-quantization-basics/)
**Perfect for beginners** - Learn fundamental concepts with hands-on implementations.

- **Concepts**: Absmax, Zero-point, LLM.int8()
- **Visualization**: Weight distributions, quantization impact
- **Evaluation**: Perplexity metrics, quality comparison
- **Duration**: ~15 minutes

[‚Üí Open Notebook](./notebooks/weight-quantization-basics/Introduction-to-Weight-Quantization.ipynb)

---

### 2. üìä [4-bit Quantization with GPTQ](./notebooks/gptq/)
**Production-ready** - Gradient-aware quantization for minimal accuracy loss.

- **Method**: GPTQ (Gradient Post-Training Quantization)
- **Precision**: 4-bit with group quantization
- **Size Reduction**: ~75% (7B ‚Üí 1.75GB)
- **Best For**: Production deployments, HuggingFace ecosystem
- **Duration**: ~60-90 minutes

[‚Üí Open Notebook](./notebooks/gptq/4-bit-LLM-Quantization-with-GPTQ.ipynb)

---

### 3. üíª [GGUF with llama.cpp](./notebooks/gguf-llama-cpp/)
**Most versatile** - Cross-platform quantization for CPU/GPU/Metal inference.

- **Format**: GGUF (GPT-Generated Unified Format)
- **Methods**: Q2_K, Q3_K, Q4_K_M, Q5_K_M, Q6_K, Q8_0
- **Platforms**: CPU, CUDA, Metal (Mac), OpenCL
- **Best For**: Desktop deployment, edge devices, Ollama
- **Duration**: ~30-45 minutes

**Recommended**: Q5_K_M for best quality/size balance

[‚Üí Open Notebook](./notebooks/gguf-llama-cpp/Quantize-Llama-2-models-using-GGUF-and-llama.cpp.ipynb)

---

### 4. ‚ö° [ExLlamaV2 Quantization](./notebooks/exllamav2/)
**Fastest inference** - Optimized for NVIDIA GPUs with maximum speed.

- **Method**: ExLlamaV2 format
- **Performance**: Fastest inference library available
- **Bits Per Weight**: Configurable (3.0-6.0 BPW)
- **Best For**: NVIDIA GPUs, speed-critical applications
- **Duration**: ~30-60 minutes

[‚Üí Open Notebook](./notebooks/exllamav2/Quantize-models-with-ExLlamaV2.ipynb)

---

## üöÄ Quick Start

### Option 1: Docker (Recommended)

```bash
# Clone repository
git clone https://github.com/your-username/LLM-Quantization-Zoo.git
cd LLM-Quantization-Zoo

# Build and start containers
make docker-build
make docker-up

# Access Jupyter at http://localhost:8888
```

### Option 2: Local Installation with Poetry

```bash
# Clone repository
git clone https://github.com/your-username/LLM-Quantization-Zoo.git
cd LLM-Quantization-Zoo

# Install Poetry (if not installed)
curl -sSL https://install.python-poetry.org | python3 -

# Install dependencies
poetry install

# Start Jupyter
poetry run jupyter notebook
```

### Option 3: Local Installation with pip

```bash
# Clone repository
git clone https://github.com/your-username/LLM-Quantization-Zoo.git
cd LLM-Quantization-Zoo

# Create virtual environment
python3 -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Start Jupyter
jupyter notebook
```

### Option 4: Automated Setup

```bash
# Run setup script
./scripts/setup.sh

# Activate virtual environment
source venv/bin/activate

# Start Jupyter
./scripts/run_notebooks.sh
```

## üê≥ Docker Support

Full Docker support with NVIDIA GPU acceleration:

```bash
# Build image
docker-compose build

# Start container
docker-compose up -d

# View logs
docker-compose logs -f

# Access shell
docker-compose exec llm-quantization-zoo bash

# Stop container
docker-compose down
```

**Features:**
- ‚úÖ NVIDIA CUDA 12.1 support
- ‚úÖ Pre-configured Jupyter environment
- ‚úÖ Persistent volumes for data
- ‚úÖ HuggingFace model cache
- ‚úÖ 16GB shared memory

## üìÅ Project Structure

```
LLM-Quantization-Zoo/
‚îÇ
‚îú‚îÄ‚îÄ notebooks/                          # All Jupyter notebooks
‚îÇ   ‚îú‚îÄ‚îÄ exllamav2/                     # ExLlamaV2 quantization
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ README.md
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ Quantize-models-with-ExLlamaV2.ipynb
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ gguf-llama-cpp/                # GGUF & llama.cpp
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ README.md
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ Quantize-Llama-2-models-using-GGUF-and-llama.cpp.ipynb
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ gptq/                          # GPTQ quantization
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ README.md
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ 4-bit-LLM-Quantization-with-GPTQ.ipynb
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ weight-quantization-basics/    # Fundamentals
‚îÇ       ‚îú‚îÄ‚îÄ README.md
‚îÇ       ‚îî‚îÄ‚îÄ Introduction-to-Weight-Quantization.ipynb
‚îÇ
‚îú‚îÄ‚îÄ scripts/                           # Helper scripts
‚îÇ   ‚îú‚îÄ‚îÄ setup.sh                       # Environment setup
‚îÇ   ‚îî‚îÄ‚îÄ run_notebooks.sh               # Start Jupyter
‚îÇ
‚îú‚îÄ‚îÄ docs/                              # Additional documentation
‚îÇ
‚îú‚îÄ‚îÄ data/                              # Data directory (git-ignored)
‚îÇ   ‚îú‚îÄ‚îÄ models/                        # Downloaded models
‚îÇ   ‚îú‚îÄ‚îÄ datasets/                      # Calibration datasets
‚îÇ   ‚îî‚îÄ‚îÄ outputs/                       # Quantized models
‚îÇ
‚îú‚îÄ‚îÄ Dockerfile                         # Docker configuration
‚îú‚îÄ‚îÄ docker-compose.yml                 # Docker Compose setup
‚îú‚îÄ‚îÄ pyproject.toml                     # Poetry configuration
‚îú‚îÄ‚îÄ requirements.txt                   # Pip requirements
‚îú‚îÄ‚îÄ Makefile                          # Convenience commands
‚îú‚îÄ‚îÄ .gitignore                        # Git ignore rules
‚îú‚îÄ‚îÄ LICENSE                           # MIT License
‚îî‚îÄ‚îÄ README.md                         # This file
```

## üõ†Ô∏è Makefile Commands

```bash
make help              # Show all available commands
make install           # Install dependencies with pip
make install-poetry    # Install dependencies with Poetry
make docker-build      # Build Docker image
make docker-up         # Start Docker containers
make docker-down       # Stop Docker containers
make docker-logs       # View container logs
make clean             # Clean cache and temporary files
make format            # Format code with black
make lint              # Run linting checks
```

## üìã Requirements

### System Requirements
- **Python**: 3.10 or higher
- **RAM**: 16GB minimum (32GB recommended)
- **GPU**: NVIDIA GPU with 8GB+ VRAM (optional but recommended)
- **Storage**: 50GB+ free space

### Core Dependencies
- PyTorch >= 2.0.0
- Transformers >= 4.35.0
- Accelerate >= 0.25.0
- bitsandbytes >= 0.41.0
- Jupyter Notebook

### Optional Dependencies
- `auto-gptq`: For GPTQ quantization
- `exllamav2`: For ExLlamaV2 (install from GitHub)
- `llama.cpp`: For GGUF quantization (requires compilation)

## üéì Learning Path

**Recommended order for learning:**

1. **Start here**: [Weight Quantization Basics](./notebooks/weight-quantization-basics/) - Understand fundamentals
2. **Next**: [GPTQ](./notebooks/gptq/) - Learn gradient-aware quantization
3. **Then**: [GGUF/llama.cpp](./notebooks/gguf-llama-cpp/) - Explore cross-platform deployment
4. **Finally**: [ExLlamaV2](./notebooks/exllamav2/) - Master high-performance inference

## üìä Comparison Matrix

| Feature | GPTQ | GGUF | ExLlamaV2 | INT8 |
|---------|------|------|-----------|------|
| **Precision** | 4-bit | 2-8 bit | 3-6 bit | 8-bit |
| **Quality** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| **Speed** | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê |
| **CPU Support** | ‚úÖ | ‚úÖ‚úÖ‚úÖ | ‚ùå | ‚úÖ |
| **GPU Support** | ‚úÖ | ‚úÖ | ‚úÖ‚úÖ‚úÖ | ‚úÖ |
| **Easy to Use** | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| **Size Reduction** | 75% | 60-85% | 65-80% | 50% |

## ü§ù Contributing

Contributions are welcome! Please feel free to:

- üêõ Report bugs
- üí° Suggest new quantization methods
- üìù Improve documentation
- ‚ú® Add new features

## üìÑ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## üåü Acknowledgments

This project builds upon excellent work from the open-source community:

- [HuggingFace Transformers](https://github.com/huggingface/transformers)
- [llama.cpp](https://github.com/ggerganov/llama.cpp) by Georgi Gerganov
- [ExLlamaV2](https://github.com/turboderp/exllamav2) by turboderp
- [AutoGPTQ](https://github.com/PanQiWei/AutoGPTQ)
- [bitsandbytes](https://github.com/TimDettmers/bitsandbytes) by Tim Dettmers

## üìß Contact

**Author**: Hao Hoang  
**GitHub**: [@your-username](https://github.com/your-username)  
**Email**: your.email@example.com

---

<div align="center">

**If this repository helped you, please consider giving it a ‚≠ê!**

Made with ‚ù§Ô∏è for the ML community

</div>
